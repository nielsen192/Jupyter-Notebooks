{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent Agents - Artificial Intelligence\n",
    "\n",
    "In this notebook we will make the concept of **rational agents** more concrete. We will see how rationality can be applied to a wide variety of agents operating in any imaginable environment.\n",
    "The plan here is to use this concept to develop a small set of design principles for building sucessful agents--systems that can reasonably be called **intelligent**.\n",
    "We start by looking at the coupling between agents, events and environments. Since some agents behave better than others, that naturally leads to the notion of agents behaving rationally and getting a more \"correct\" result. How well they behave though will depend upon things like the environment they run in. Some environments are just much harder to work through.\n",
    "\n",
    "## Agents and Environments\n",
    "\n",
    "- Agent = anything that can be viewed as perceiving its environment through     sensors and acting upon that environment through actuators.\n",
    "- Human agent = organs for sensors, hands legs etc. for actuators.\n",
    "- Robotic agent = cameras and infrared range finders for sensors, various motors for actuators\n",
    "- Software agent = receives keystrokes, file contents and network packets as sensory; and acts on it\n",
    "- Percept Sequence = the complete history of everything the agent has ever perceived\n",
    "- An agent program runs in cycles of: (1) Perceive, (2) Think, and (3) Act\n",
    "\n",
    "**Agent Function is an abstract mathematical description; The Agent Program is a concrete implementation** \n",
    "\n",
    "## Good Behavior: The Concept of Rationality\n",
    "\n",
    "- Rational agent = One that does the right thing, conceptually speaking\n",
    "How do we know what is right behavior? We look at the **consequences!**\n",
    "When an agent goes through an environment, it generates a sequence of actions according to the percepts it receives. This sequence of actions causes the environment to go through a sequence of states. If the state is desirable, then the agent has performed well.\n",
    "**Desirability = Performance Measure that evaluates any given sequence of environment states**\n",
    "It is important to note here that it is **Environment states** and not agent states we are looking at. If we were looking at agent states, the agent could decide that itself was perfectly rational and therefore already had a perfect performance measure (a bit like most people). This is something we must avoid.\n",
    "\n",
    "- General rule: Design Performance Measures according to what you want in the environment, rather than how you think the agent should behave.\n",
    "\n",
    "### Rationality\n",
    "\n",
    "What is rationality at any given time depends on four things:\n",
    "* The performance measure that defines the criterion of success\n",
    "* The agent's prior knowledge of the environment\n",
    "* The actions that the agent can perform\n",
    "* The agent's percept sequence to date\n",
    "\n",
    "**Definition of a rational agent**\n",
    "*For each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has.*\n",
    "\n",
    "**Remember the notion of penalty points (minus points) for the performance measure**\n",
    "\n",
    "**Rationality maximizes EXPECTED performance. Perfection maximizes ACTUAL performance**\n",
    "\n",
    "Sometimes it is needed to do some information gathering and/or some exploration in order to properly make a rational agent.\n",
    "It is all about **learning** from **perceiving**. \n",
    "\n",
    "- Experience = Perceiving + Learning\n",
    "\n",
    "In nature it is seen in multiple places that animals, especially insects, have difficulty with adapting to a change in their behaviors. Now this can be a problem. If an agent relies too much or even purely on pre-existing knowledge of its designer rather than its own percepts, we say it lacks **autonomy**. A rational agent should be autonomous - it should learn what it can to compensate for partial or incorrect knowledge.\n",
    "In most practical cases an agent wouldn't be completely autonomous from the beginning. This could create a randomly acting agent. However this might be what you want in some experiments.\n",
    "\n",
    "### Task Environments\n",
    "\n",
    "Task Environments are essentially the \"problems\" for which the agents are the \"solutions\".\n",
    "\n",
    "#### Specifying the task environment\n",
    "\n",
    "With the vacuum cleaner example we had to specify the performance measure, the environment, and the agent's actuators and sensors. This together is called **PEAS** (Performance, Environment, Actuators, Sensors). When designing an agent, the first step must always be to specify the task environment as fully as possible.\n",
    "Let's look at a more complex example, for which there is no real possibility of right now. An automated taxi driver. The table below summarizes the PEAS description for the taxi's task environment.\n",
    "\n",
    "![TaxiPEAS](TaxiPEAS.png)\n",
    "\n",
    "#### Properties of task environments\n",
    "\n",
    "In the table below are listed a few informal descriptions of the different agent types and their PEAS descriptions. They will be covered in depth later.\n",
    "Even though there are a wealth of possibilities we can generalize it a little like this. First we list the dimensions, then we analyze several task environments to illustrate the ideas.\n",
    "\n",
    "![AgentTypesAndPEAS](AgentTypesAndPEAS.png)\n",
    "\n",
    "**Types of Environments**\n",
    "\n",
    "- Fully Observable: agent's sensors give it access to the complete state of the environment at each point in time.\n",
    "- Partially Observable: might be because of noisy and inaccurate sensors or because parts of the state are missing from the data. Taxi driver can't know what other drivers are thinking.\n",
    "- Unobservable: agent with no sensors. Goal may still be achievable.\n",
    "- Single agent: example - agent playing sudoku with itself\n",
    "- multi-agent: example - agent playing chess (two player environment)\n",
    "Now we run into a problem though. When does agent A have to treat B as an agent and not just as an object reacting to the laws of physics? It all has to do with the action of maximizing performance measure. In chess you get Competitive Multiagent Environment. In the taxi driver example, we end up with a partially Cooperative and a partially Competitive multiagent environment. Often times in those cases, communication emerges as a rational behavior; in some competitive environments, randomized behavior is rational because it avoids the pitfalls of predictability.\n",
    "- Deterministic: next state completely determined by current state and agent action\n",
    "- Stochastic: next state not completely determined by current state and agent action. This is the real world situation because of all the unobserved\n",
    "- Uncertain: if not fully observable or not deterministic. note: \"stochastic\" generally implies that uncertainty about outcomes is quantified in terms of probabilities\n",
    "- Nondeterministic: one in which actions are characterized by their possible outcomes, but without probabilities attached to them. Usually associated with performance measures that requires the agent to succeed for all possible outcomes of its actions.\n",
    "- Episodic: experience is divided into atomic episodes. In each episode the agent receives a percept and then performs a single action. Crucially, the next episode does not depend on the actions taken in the previous episodes.\n",
    "- Sequential: current decision could affect all future decisions. \"The butterfly effect\"\n",
    "- Static: the environment doesn't change while an agent is deliberating(considering)\n",
    "- Dynamic: the environment can change while an agent is deliberating\n",
    "- SemiDynamic: environment doesn't change with the passage of time, but the agents performance score does\n",
    "- Discrete vs Continuous: applies to the state of the environment, to the way time is handled, and to the percepts and actions of the agent. For example, the chess environment has a finite number of distinct states. Chess also has a discrete set of percepts and actions. Taxi driving is a continuous-state and continuous-time problem: the speed and location of the taxi and of the other vehicles sweep through a range of continuous values and do so smoothly over time. Taxi-driving actions are also continuous (steering angles, etc.). Input from digital cameras is discrete, strictly speaking, but is typically treated as representing continuously varying intensities and locations.\n",
    "- Known: outcomes for all actions are given. Refers to the agent's state of knowledge about the \"laws of physics\" of the environment.\n",
    "- Unknown: agent will have to learn how it(environment) works in order to make good decisions\n",
    "\n",
    "**Examples of task environments and their characteristics**\n",
    "![TEChar](TEChar.png)\n",
    "\n",
    "### Agent Types\n",
    "\n",
    "Four basic types in order of increasing generality:\n",
    "- Simple reflex agents\n",
    "- Model-based reflex agents\n",
    "- Goal-based agents\n",
    "- Utility-based agents\n",
    "\n",
    "They can all be generalized into **learning** agents that can improve their performance and generate better actions.\n",
    "\n",
    "Simple Reflex agents:\n",
    "- select actions based only on current state. Ignore percept history\n",
    "- simple but limited\n",
    "- only work if environment is fully observable. Correct action is based on current percept only.\n",
    "example: Vacuum table\n",
    "function vacuumagent(location, state)\n",
    "    return action\n",
    "        if status = Dirty then return suck\n",
    "        else if location = A then return right\n",
    "            else return left\n",
    "\n",
    "\n",
    "Model-based Reflex agents:\n",
    "- handle partial observability by keeping track of the part of the world it can't see now\n",
    "- internal state depending on the percept history (best guess)\n",
    "- model of the world based on (1) how the world evolves independently from the agent, and (2) how the agent actions affects the world\n",
    "\n",
    "\n",
    "Goal-based agents:\n",
    "- knowing the current state of the environment is not enough. the agent needs some goal info\n",
    "- agent program combines goal information with the environment model to choose the actions that achieve that goal.\n",
    "- consider the future with \"what will happen if i do A?\"\n",
    "- flexible as knowledge supporting the decisions is explicitly represented and can be modified\n",
    "\n",
    "Utility-based agents:\n",
    "- sometimes achieving the goals is not enough. we may look for quicker, safer, cheaper trip to reach a destination\n",
    "- agent happiness should be taken into consideration. we call it **utility**\n",
    "- a utility function is the agent's performance measure\n",
    "- because of the uncertainty in the world, a utility agent chooses the action that maximizes the expected utility\n",
    "\n",
    "\n",
    "#### Learning Agents\n",
    "Programming agents by hand can be very tedious. \"Some more expeditious method seem desirable\" Alan Turing, 1950.\n",
    "\n",
    "Four conceptual components:\n",
    "- Learning element: responsible for making improvements\n",
    "- Performance element: responsible for selecting external actions. It is what we considered as an agent so far\n",
    "- Critic: how well is the agent doing with regards to a fixed performance standard\n",
    "- Problem generator: allows the agent to explore\n",
    "\n",
    "### Agent's organization\n",
    "\n",
    "a) Atomic Representation: Each state of the world is a black-box that has no internal structure. E.g., finding a driving route, each state is a city. AI algorithms: search, games, Markov decision processes, hidden Markov models, etc.\n",
    "\n",
    "b) Factored Representation: Each state has some attribute-value properties. E.g., GPS location, amount of gas in the tank. AI algorithms: constraint satisfaction, and Bayesian networks.\n",
    "\n",
    "c) Structured Representation: Relationships between the objects of a state can be explicitly expressed. AI algorithms: first order logic, knowledge-based learning, natural language understanding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
